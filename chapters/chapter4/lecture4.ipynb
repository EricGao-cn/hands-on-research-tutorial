{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Tensor\n",
        "我们将从最基本的张量开始。首先，浏览官方张量教程[这里](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)。\n",
        "\n",
        "> 张量是一种特殊的数据结构，与数组和矩阵非常相似。在PyTorch中，我们使用张量对模型的输入和输出以及模型的参数进行编码。张量与NumPy的 ndarray 类似，不同之处在于张量可以在GPU或其他专用硬件上运行以加速计算。如果您熟悉 ndarrays，那么您就会熟悉Tensor API。如果没有，请按照此下面的问题进行操作。最好可以不看答案操作一遍，先思考一下，再去搜索一下，最后比对一下正确的操作。这样子效果是最好的。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6rCGrecBBuY"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJg3x_jFLXgp"
      },
      "source": [
        "1. 将二维列表 [[5,3], [0,9]] 转换为一个张量"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9lTGGkvLNvJ"
      },
      "outputs": [],
      "source": [
        "data = [[5, 3], [0, 9]]\n",
        "x_data = torch.tensor(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPho5cJLmzA"
      },
      "source": [
        "2. 使用区间 [0, 1) 上均匀分布的随机数创建形状 (5, 4) 的张量“t”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwdJxM-XLo4l"
      },
      "outputs": [],
      "source": [
        "t = torch.rand((5,4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-3-cxcyLvDJ"
      },
      "source": [
        "3. 找出张量“t”所在的设备及其数据类型。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kRPyprDMc1N",
        "outputId": "e5e4560d-ebee-4a3d-93c0-176461aa05e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "print(t.device) # cpu\n",
        "\n",
        "print(t.dtype) # float32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEZEMKXpL6ON"
      },
      "source": [
        "4. 创建形状 (4,4) 和 (4,4) 的两个随机张量，分别称为“u”和“v”。将它们连接起来形成形状为 (8, 4) 的张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj7ksR69Mkh0",
        "outputId": "d28f40ec-19d6-41ee-e358-2afecbdaf3df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4])\n"
          ]
        }
      ],
      "source": [
        "u = torch.randn((4,4))\n",
        "v = torch.randn((4,4))\n",
        "print(torch.concat((u,v), dim=0).shape) # torch.Size([8, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TzX8BNKL-1y"
      },
      "source": [
        "5. 连接 u 和 v 以创建形状 (2, 4, 4) 的张量。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Tk2CDLGMoQq",
        "outputId": "d6fa6f95-b2d6-4e76-9c68-93bbc9333ce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 4, 4])\n"
          ]
        }
      ],
      "source": [
        "print(torch.stack((u,v), dim=0).shape) # torch.Size([2, 4, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYdfN7TKMDf-"
      },
      "source": [
        "6. 连接 u 和 v 形成一个张量，称为形状 (4, 4, 2) 的 w。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q10UET4NMqzf",
        "outputId": "1451cb85-00ce-412d-ea3d-04cd5658684e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 4, 2])\n"
          ]
        }
      ],
      "source": [
        "w = torch.stack((u,v), dim=2)\n",
        "print(w.shape) # torch.Size([4, 4, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stR-Or2jMI8e"
      },
      "source": [
        "7. 索引 w 位于 3, 3, 0。将该元素称为“e”。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNmo0-L5MtNb"
      },
      "outputs": [],
      "source": [
        "e = w[3,3,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXjIhEQxMPBt"
      },
      "source": [
        "8. 会在 u 或 v 的哪一个中找到 w？并核实。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6y1zAxuJM5M-",
        "outputId": "54bd7dff-6fe5-4077-fc52-bb4714d048c8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# in u\n",
        "w[3,3,0] == u[3,3] # True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rZPsa-4MSWC"
      },
      "source": [
        "9. 创建一个形状为 (4, 3) 的全为 1 的张量 ‘a’。对 ‘a’ 进行元素级别的自乘操作。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSjcPCOaM_aa",
        "outputId": "8d2f0c5e-9669-43ba-9dbd-14b3cc7ce850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a = torch.ones((4,3))\n",
        "a * a # tensor([[1., 1., 1.],[1., 1., 1.],[1., 1., 1.],[1., 1., 1.]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OR05FJOMVf_"
      },
      "source": [
        "10. 向“a”添加一个额外的维度（新的第 0 维度）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Osk8Cj0cNCUf",
        "outputId": "393f215c-e8c3-49bd-d6e3-bc64e38fb566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 3])\n"
          ]
        }
      ],
      "source": [
        "print(torch.unsqueeze(a, 0).shape) # torch.Size([1, 4, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWxmW68kMXdN"
      },
      "source": [
        "11. 执行 a 与转置矩阵的乘法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyfW3pN6NEnv",
        "outputId": "d9d4b3bc-0529-4d64-de82-4cd936cdda84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[3., 3., 3., 3.],\n",
              "        [3., 3., 3., 3.],\n",
              "        [3., 3., 3., 3.],\n",
              "        [3., 3., 3., 3.]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a @ a.T # tensor([3., 3., 3., 3.],[3., 3., 3., 3.],[3., 3., 3., 3.],[3.,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpZl-U8ZMZ4Y"
      },
      "source": [
        "12. a.mul(a) 会产生什么结果？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvgoz80iNHKl"
      },
      "outputs": [],
      "source": [
        "# An elementwise multiplication, same as #9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-R7_MakNXrs"
      },
      "source": [
        "13. a.matmul(a.T) 会产生什么结果？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb4ENaw4L4nH"
      },
      "outputs": [],
      "source": [
        "# A matrix multiplication aka dot product, same as #11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2GxhgSKNdZs"
      },
      "source": [
        "14. What would a.mul(a.T) result in?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhcIBCu0Ngx1"
      },
      "outputs": [],
      "source": [
        "# An error; the sizes won’t match."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5EoNhQfNlst"
      },
      "source": [
        "15. 猜猜下面会打印什么。验证一下"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdFZJ7b-Nld0",
        "outputId": "fe6d14a5-fbef-4d82-f977-bf62fbde14fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([2., 1., 1., 1., 1.])\n"
          ]
        }
      ],
      "source": [
        "t = torch.ones(5)\n",
        "n = t.numpy()\n",
        "n[0] = 2\n",
        "print(t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_kBM2hHN3g9"
      },
      "source": [
        "16. 下面会打印什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AOuR0xdsN_Pv"
      },
      "outputs": [],
      "source": [
        "t = torch.tensor([2., 1., 1., 1., 1.])\n",
        "t.add(2)\n",
        "t.add_(1)\n",
        "print(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.Autograd 和神经网络\n",
        "接下来，我们学习[自动梯度](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)教程和[神经网络](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html)教程。\n",
        "\n",
        "神经网络(NN) 是对某些输入数据执行的嵌套函数的集合。这些函数由参数（由权重和偏差组成）定义，这些参数在PyTorch中存储在张量中。可以使用 torch.nn 包构建神经网络。\n",
        "\n",
        "训练神经网络分两步进行：\n",
        "\n",
        "- 前向传播：在前向传播中，神经网络对正确的输出做出最佳猜测。它通过每个函数运行输入数据来进行猜测。\n",
        "- 反向传播：在反向传播中，神经网络根据其猜测的误差按比例调整其参数。它通过从输出向后遍历、收集误差相对于函数参数（梯度）的导数并使用梯度下降来优化参数来实现这一点。\n",
        "\n",
        "更一般地，神经网络的典型训练过程如下：\n",
        "\n",
        "- 定义具有一些可学习参数（或权重）的神经网络\n",
        "- 迭代输入数据集\n",
        "- 通过网络处理输入\n",
        "- 计算损失（输出距离正确还有多远）\n",
        "- 将梯度传播回网络参数\n",
        "- 更新网络的权重，通常使用简单的更新规则：权重=权重-学习率梯度\n",
        "\n",
        "有了这些教程，我们就可以尝试以下练习了！假设我们有以下起始代码，将下面这段代码复制到你的编辑器中："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "data = torch.rand(1, 3, 64, 64)\n",
        "labels = torch.rand(1, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "17. 使用数据对模型进行前向传递并将其保存为 `preds`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds = model(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "18. `preds` 的形状应该是什么？验证你的猜测。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# It should be 1 x 1000\n",
        "preds.shape # torch.Size([1, 1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "19. 将 `resnet18` 的 `conv1` 属性的权重参数保存为 `w`。打印 `w` 因为我们稍后需要它（请注意，我的 `w` 不会与你的相同）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w = model.conv1.weight\n",
        "print(w) # tensor([[[[-1.0419e-02,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "20. `w` 的 `grad` 属性应该是什么？请验证。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Should be None. That’s because we haven’t run backward yet.\n",
        "print(w.grad) # None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "21. 创建一个[交叉熵](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)损失对象，并用它来使用 `labels` 和 `preds` 计算损失，保存为 `loss`。打印 `loss`，因为我们稍后需要它。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ce = torch.nn.CrossEntropyLoss()\n",
        "loss = ce(preds, labels)\n",
        "print(loss) # tensor(3631.9521, grad_fn=<DivBackward1>)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "22. 打印最后一次产生 `loss` 损失的数学运算。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(loss.grad_fn) # <DivBackward1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "23. 执行反向传播。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "24. `w` 应该改变吗？检查 #3 的输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# No"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "25. `w` 的 `grad` 属性会与 #4 不同吗？并验证。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yes\n",
        "print(w.grad) # tensor([[[[ 7.0471e+01,  5.9916e+00,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "26. `loss` 的 `grad` 属性应该返回什么？验证一下。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 返回None值，在深度学习中，我们通常会计算损失（loss）并进行反向传播来更新模型的参数。这里提到的“loss”指的是计算出来的损失值，它不是模型参数中的一个叶子节点（叶子节点是指那些需要更新的参数）。\n",
        "\n",
        "# 因为这个损失值不是叶子节点，所以默认情况下，它不会保存反向传播时计算出的梯度。如果我们想要保存这些梯度，就需要调用 loss.retain_grad() 方法来明确告诉系统“即使这个损失值不是叶子节点，也请保留它的梯度”。\n",
        "\n",
        "# 由于我们没有调用这个方法，所以在进行反向传播时，损失值的梯度不会被保存。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "27. `loss` 的 `requires_grad` 属性应该是什么？验证一下。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# True\n",
        "print(loss.requires_grad) # True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "28. `labels` 的 `requires_grad` 属性应该是什么？验证一下。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# False\n",
        "print(labels.requires_grad) # False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "29. 如果你再次执行反向传播会发生什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 会发生运行时错误，因为在第一次调用 .backward() 时，计算图中保存的中间值会被释放，除非我们指定 retain_graph=True。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "30. 创建一个学习率 (`lr=1e-2`) 和动量 (`momentum=0.9`) 的 [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) 优化器对象，并执行一步。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sgd = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
        "sgd.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "31. `w` 是否应该改变？检查第3题的输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Yes (step changes the parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "32. `loss` 是否应该改变？检查第5题的输出。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 不会（因为它不是模型参数的一部分）。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "33. 将所有可训练参数的梯度清零。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "34. `w` 的 `grad` 属性应该是什么？验证一下。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "35. 在不运行的情况下，判断以下代码是否会成功执行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data1 = torch.zeros(1, 3, 64, 64)\n",
        "data2 = torch.ones(1, 3, 64, 64)\n",
        "\n",
        "predictions1 = model(data1)\n",
        "predictions2 = model(data2)\n",
        "l = torch.nn.CrossEntropyLoss()\n",
        "loss1 = l(predictions1, labels)\n",
        "loss2 = l(predictions2, labels)\n",
        "\n",
        "loss1.backward()\n",
        "loss2.backward()\n",
        "\n",
        "#可以执行成功，当计算图的中间值被释放时，loss2.backward() 将无法工作；然而，我们并未对 loss2 使用相同的中间值，所以它将能够工作。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "36. 判断以下代码是否会成功执行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "data1 = torch.zeros(1, 3, 64, 64)\n",
        "data2 = torch.ones(1, 3, 64, 64)\n",
        "\n",
        "predictions1 = model(data1)\n",
        "predictions2 = model(data1)\n",
        "\n",
        "l = torch.nn.CrossEntropyLoss()\n",
        "loss1 = l(predictions1, labels)\n",
        "loss2 = l(predictions2, labels)\n",
        "\n",
        "loss1.backward()\n",
        "loss2.backward()\n",
        "\n",
        "# 可以执行成功，当计算图的中间值被释放时，loss2.backward() 将无法工作；然而，我们并未对 loss2 使用相同的中间值，所以它将能够正常工作。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "37. 判断以下代码是否会成功执行。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data1 = torch.zeros(1, 3, 64, 64)\n",
        "data2 = torch.ones(1, 3, 64, 64)\n",
        "\n",
        "predictions1 = model(data1)\n",
        "predictions2 = model(data2)\n",
        "\n",
        "l = torch.nn.CrossEntropyLoss()\n",
        "loss1 = l(predictions1, labels) # 注意是predictions1\n",
        "loss2 = l(predictions1, labels) # 注意是predictions1\n",
        "\n",
        "loss1.backward()\n",
        "loss2.backward()\n",
        "\n",
        "# 不会成功，当计算图的中间值被释放时，loss2.backward() 将无法工作；在这里，predictions1 的中间值将已被释放。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "38. 对于不能执行的代码，你如何修改其中一个 `.backward` 行使其工作？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 将第一次调用 .backward() 改为使用 retain_graph=True。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "39. 以下代码的输出是什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "predictions1 = model(data)\n",
        "l = torch.nn.CrossEntropyLoss()\n",
        "loss1 = l(predictions1, labels)\n",
        "loss1.backward(retain_graph=True)\n",
        "\n",
        "w = model.conv1.weight.grad[0][0][0][0]\n",
        "a = w.item()\n",
        "\n",
        "loss1.backward()\n",
        "b = w.item()\n",
        "\n",
        "model.zero_grad()\n",
        "c = w.item()\n",
        "\n",
        "print(b//a,c)\n",
        "\n",
        "# 2.0, 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "40. 以下代码的输出是什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "predictions1 = model(data)\n",
        "l = torch.nn.CrossEntropyLoss()\n",
        "loss1 = l(predictions1, labels)\n",
        "loss1.backward(retain_graph=True)\n",
        "\n",
        "a = model.conv1.weight.grad[0][0][0][0]\n",
        "\n",
        "loss1.backward()\n",
        "b = model.conv1.weight.grad[0][0][0][0]\n",
        "\n",
        "model.zero_grad()\n",
        "c = model.conv1.weight.grad[0][0][0][0]\n",
        "\n",
        "print(b//a,c)\n",
        "\n",
        "# tensor(nan) 和 tensor(0)。因为 a、b、c 都引用了相同的数据。没有使用 item()。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "41. 以下代码有什么问题？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "learning_rate = 0.01\n",
        "for f in net.parameters():\n",
        "    f.data.sub(f.grad.data * learning_rate)\n",
        "\n",
        "# sub 调用应该改为 sub_，这样才能正确地执行预期的原地操作。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "42. 按正确的顺序排列训练循环的以下步骤（有多种正确答案，但你在教程中会看到一种典型的设置）：以下代码的输出是什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# optimizer.step(), optimizer.zero_grad(), loss.backward(), output = net(input), loss = criterion(output, target)\n",
        "# 这是其中一种\n",
        "\n",
        "optimizer.zero_grad()\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "43. 以下代码的输出是什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "net = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
        "data = torch.rand(1, 3, 64, 64)\n",
        "target = torch.rand(1, 1000)\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "orig = net.conv1.weight.clone()[0, 0, 0, 0]\n",
        "weight = net.conv1.weight[0, 0, 0, 0]\n",
        "# 1\n",
        "optimizer.zero_grad()\n",
        "print(f\"{weight == orig}\")\n",
        "\n",
        "# 2\n",
        "output = net(data)\n",
        "loss = criterion(output, target)\n",
        "print(f\"{weight == orig}\")\n",
        "\n",
        "# 3\n",
        "loss.backward()\n",
        "print(f\"{weight == orig}\")\n",
        "\n",
        "# 4\n",
        "optimizer.step()\n",
        "print(f\"{weight == orig}\")\n",
        "\n",
        "#True\n",
        "#True\n",
        "#True\n",
        "#False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "44. 我们将实现一个有一个隐藏层的神经网络。这个网络将接受一个32x32的灰度图像输入，展开它，通过一个有100个输出特征的仿射变换，应用 `ReLU` 非线性，然后映射到目标类别（10）。实现初始化和前向传递，完成以下代码。使用 `nn.Linear`, `F.relu`, `torch.flatten`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # 在这里补充代码\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 在这里补充代码\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "45. 用两行代码验证你能通过上述网络进行前向传递。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = Net()\n",
        "preds = net.forward(torch.randn(1, 1, 32, 32))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "45. 在不运行代码的情况下，猜测以下语句的结果是什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "net = Net()\n",
        "print(len(list(net.parameters())))\n",
        "# 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "47. 获取网络参数的名称"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print([name for name, _ in net.named_parameters()]) # ['fc1.weight', 'fc1.bias', 'fc2.weight', 'fc2.bias']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "48. 以下语句指的是哪个网络层？它将评估什么？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(list(net.parameters())[1].size())\n",
        "# Fc1.bias. torch.Size([100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "49. 以下示意图包含了实现一个神经网络所需的所有信息。实现初始化和前向传递，完成以下代码。使用 `nn.Conv2d`, `nn.Linear`, `F.max_pool2d`, `F.relu`, `torch.flatten`。提示：`ReLU` 在子采样操作后和前两个全连接层之后应用。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # your code here\n",
        "        return x\n",
        "\n",
        "# 以下参考答案\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # your code here\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = F.relu(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "50.修改上述代码，使用 `nn.MaxPool2d` 代替 `F.max_pool2d`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.maxpool = nn.MaxPool2d(2, 2) # 替换\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "51. 尝试通过将第一个卷积层的输出通道数从6增加到12来增加网络的宽度。怎么修改？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 以下只展示init函数\n",
        "def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 12, 5)\n",
        "    self.conv2 = nn.Conv2d(12, 16, 5) # 这里我们也可以修改输入的通道\n",
        "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    self.maxpool = nn.MaxPool2d(2, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.分类器训练"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "接下来，我们进入教程的最后一部分：[Cifar10教程](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)。这个教程通过以下步骤来训练一个图像分类器：\n",
        "\n",
        "- 使用torchvision加载和归一化 (normalize) CIFAR10训练和测试数据集 \n",
        "- 定义一个卷积神经网络 \n",
        "- 定义一个损失函数\n",
        "- 在训练数据上训练网络\n",
        "- 在测试数据上测试网络\n",
        "\n",
        "完成上述教程后，回答以下问题："
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "52. 以下数据集加载代码可以运行，但代码中是否有错误？这些错误的影响是什么？如何修复这些错误？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "transform = transforms.Compose(\n",
        "   [transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=True,\n",
        "                                      download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                        shuffle=False, num_workers=2)\n",
        "\n",
        "# 有两个错误。首先，我们没有对训练数据加载器进行随机打乱。其次，我们在测试集中加载了 CIFAR 的训练数据，而在训练集中加载了 CIFAR 的测试数据。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "53. 编写两行代码从数据加载器中获取随机的训练图像（假设上面的错误已经修复）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "54. 以下训练代码可以运行，但代码中是否有错误（包括计算效率低下）？这些错误的影响是什么？如何修复这些错误？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "running_loss = 0.0\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    # get the inputs; data is a list of [inputs, labels]\n",
        "    inputs, labels = data\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    outputs = net(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print statistics\n",
        "    running_loss += loss\n",
        "    if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "        running_loss = 0.0\n",
        "        break\n",
        "\n",
        "# 有两个错误。首先，循环中应该有一个 optimizer.zero_grad()。没有这个步骤，梯度将会累积。其次，running_loss 应该使用 loss.item() 进行累加；否则，每个损失仍然会是计算图的一部分，这会占用内存，因为否则各个损失值会被垃圾回收。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "55. 以下评估代码可以运行，但其中是否存在错误（包括计算效率低下）？这些错误的影响是什么？如何修正这些错误？"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    # calculate outputs by running images through the network\n",
        "    outputs = net(images)\n",
        "    # the class with the highest energy is what we choose as prediction\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "# 有两个错误。首先，应该在循环外用 torch.no_grad()，这将禁用自动求导引擎，减少内存使用并加速计算，但你将无法进行反向传播（在评估脚本中这是不需要的）。其次，我们再次遗漏了 sum() 后的 .item() 调用，这意味着张量仍然会是计算图的一部分，占用内存。"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
